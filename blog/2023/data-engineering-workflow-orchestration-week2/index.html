<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Data Engineering Week 2 - Workflow Orchestration | Kamal Patel</title> <meta name="author" content="Kamal Patel"> <meta name="description" content="Workflow orchestration with prefect"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon_circular.png?39e7cf776cc7ac0763b11674983db843"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kamalpatell.github.io/blog/2023/data-engineering-workflow-orchestration-week2/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://kamalpatell.github.io/"><span class="font-weight-bold">Kamal</span> Patel</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/"> Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/"> Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/"> Repositories</a> </li> <li class="nav-item "> <a class="nav-link" target="_blank" href="/assets/pdf/resume.pdf">Resume</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h2 class="post-title">Data Engineering Week 2 - Workflow Orchestration</h2> <p class="post-meta"> November 13, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/data-engineering"> <i class="fas fa-hashtag fa-sm"></i> data-engineering</a>     ·   <a href="/blog/category/data"> <i class="fas fa-tag fa-sm"></i> data</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"><ul id="toc" class="section-nav"> <li class="toc-entry toc-h3"><a href="#what-is-data-lake">What is Data Lake</a></li> <li class="toc-entry toc-h3"><a href="#data-lake-vs-data-warehouse">Data Lake vs. Data Warehouse</a></li> <li class="toc-entry toc-h3"><a href="#extract-transform-load-etl-vs-extract-load-transform-elt">Extract-Transform-Load (ETL) vs. Extract-Load-Transform (ELT)</a></li> <li class="toc-entry toc-h3"><a href="#introduction-to-data-warehouse">Introduction to data warehouse</a></li> <li class="toc-entry toc-h3"> <a href="#workflow-orchestration">Workflow Orchestration</a> <ul> <li class="toc-entry toc-h4"><a href="#what-is-dataflow">What is dataflow</a></li> <li class="toc-entry toc-h4"><a href="#introduction-to-workflow-orchestration">Introduction to workflow orchestration</a></li> </ul> </li> <li class="toc-entry toc-h3"><a href="#introduction-to-prefect-concepts">Introduction to Prefect concepts</a></li> <li class="toc-entry toc-h3"><a href="#loading-data-into-postgres-using-prefect">Loading data into Postgres using Prefect</a></li> <li class="toc-entry toc-h3"><a href="#etl-to-gcp">ETL to GCP</a></li> <li class="toc-entry toc-h3"> <a href="#prefect-deployment">Prefect Deployment</a> <ul> <li class="toc-entry toc-h4"><a href="#using-prefect-cli">Using Prefect CLI</a></li> </ul> </li> <li class="toc-entry toc-h3"> <a href="#docker-infrastructure">Docker Infrastructure</a> <ul> <li class="toc-entry toc-h4"><a href="#prefect-deployment-1">prefect deployment</a></li> </ul> </li> </ul></div> <hr> <div id="markdown-content"> <h3 id="what-is-data-lake"><strong>What is Data Lake</strong></h3> <p>A Data Lake consists of a central repository where any type of data, either structured or unstructured, can be stored. The main idea behind a Data Lake is to ingest and make data available as quickly as possible inside an organization.</p> <p>Several popular data lake solutions available, some of them are:</p> <ol> <li><code class="language-plaintext highlighter-rouge">Amazon S3</code></li> <li><code class="language-plaintext highlighter-rouge">Google Cloud Storage</code></li> <li><code class="language-plaintext highlighter-rouge">Microsoft Azure Data Lake Storage</code></li> <li><code class="language-plaintext highlighter-rouge">Hadoop HDFS</code></li> <li> <code class="language-plaintext highlighter-rouge">Snoflake Data Warehouse</code>: It’s a cloud-based data warehouse solution with data lake component.</li> </ol> <h3 id="data-lake-vs-data-warehouse"><strong>Data Lake vs. Data Warehouse</strong></h3> <p>A Data Lake stores a huge amount of data and are normally used for stream processing, machine learning and real time analytics. On the other hand, a Data Warehouse stores structured data for analytics and batch processing.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/data/datalake1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/data/datalake1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/data/datalake1-1400.webp"></source> <img src="/assets/img/blog/data/datalake1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Data Lake vs. Data Warehouse </div> <h3 id="extract-transform-load-etl-vs-extract-load-transform-elt"><strong>Extract-Transform-Load (ETL) vs. Extract-Load-Transform (ELT)</strong></h3> <p><code class="language-plaintext highlighter-rouge">ETL (Extract, Transform, Load)</code> refers to the traditional approach of extracting data from various sources, transforming the data into the required format, and then loading the data into the warehouse.</p> <p><code class="language-plaintext highlighter-rouge">ELT (Extract, Load, Transform)</code> refers to a newer approach in which the is extracted from various sources and loaded into a data lake first and then transformed and processed for analysis.</p> <p>ETL is usually a Data Warehouse solution, used mainly for small amount of data as a schema-on-write approach. On the other hand, ELT is a Data Lake solution, employed for large amounts of data as a schema-on-read approach.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/data/datalake2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/data/datalake2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/data/datalake2-1400.webp"></source> <img src="/assets/img/blog/data/datalake2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image: https://vitalflux.com/data-lake-design-principles-best-practices/ </div> <h3 id="introduction-to-data-warehouse"><strong>Introduction to data warehouse</strong></h3> <p>A data warehouse is a central repository of information that can be analyzed to make more informed decisions. Reports, dashboards, and analytics tools are indispensable for business users seeking to derive insights from their data, oversee business performance, and facilitate decision-making. The foundation of these tools lies in data warehouses, which store data efficiently to reduce input and output (I/O) operations, ensuring swift delivery of query results to a multitude of users simultaneously.</p> <p>Popular data warehouse solution available are:</p> <ol> <li> <p><code class="language-plaintext highlighter-rouge">Amazon Redshift</code>: A fully managed, petabyte-scale data warehouse service provided by Amazon Web Services (AWS). Redshift uses columnar storage, parallel query execution, and advanced compression algorithms to provide fast query performance.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Google BigQuery</code>: A serverless, highly scalable, and cost-effective cloud data warehouse provided by Google Cloud Platform. BigQuery supports SQL-like queries and provides fast query performance through its columnar storage and massive parallel processing.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Microsoft Azure Synapse Analytics (formerly SQL Data Warehouse)</code>: A cloud-based big data analytics service provided by Microsoft Azure. It combines the capabilities of data warehousing and big data analytics, allowing organizations to analyze data at scale.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Teradata</code>: A relational database management system for data warehousing and big data analytics provided by Teradata Corporation. Teradata provides a scalable, high-performance platform for storing and analyzing large amounts of structured d</p> </li> </ol> <h3 id="workflow-orchestration"><strong>Workflow Orchestration</strong></h3> <h4 id="what-is-dataflow">What is dataflow</h4> <p>A dataflow defines all extraction and processing steps that the data will be submitted to, also detailing any transformation and intermediate states of the dataset. For example, in an ETL process, a dataset is first extracted (E) from some source (e.g., website, API, etc), then transformed (T) (e.g., dealing with corrupted or missing values, joining datasets, datatype conversion, etc) and finally loaded (L) to some type of storage (e.g., data warehouse).</p> <h4 id="introduction-to-workflow-orchestration">Introduction to workflow orchestration</h4> <p>A workflow orchestration tool allows us to manage and visualize dataflows, while ensuring that they will be run according to a set of predefined rules. A good workflow orchestration tool makes it easy to schedule or execute dataflows remotely, handle faults, integrate with external services, increase reliability.</p> <p>Several tools available for workflow orchestration in data engineering are:</p> <ol> <li> <p><code class="language-plaintext highlighter-rouge">Apache Airflow</code>: An open-source platform for programmatically authoring, scheduling, and monitoring workflows.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">AWS Glue</code>: A fully managed extract, transform, and load (ETL) service offered by Amazon Web Services (AWS).</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Prefect</code>: An open-source workflow orchestration tool that provides a simple and flexible interface for building and managing data pipelines.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Luigi</code>: An open-source Python module for building complex pipelines and workflows.</p> </li> </ol> <h3 id="introduction-to-prefect-concepts"><strong>Introduction to Prefect concepts</strong></h3> <p>Prefect is an modern management systems for data-intensive workflows. It’s the simplest way to transform any Python function into a unit of work that can be observed and orchestrated.</p> <h3 id="loading-data-into-postgres-using-prefect">Loading data into Postgres using Prefect</h3> <p><strong><code class="language-plaintext highlighter-rouge">Step 1</code></strong></p> <p>Create conda environment to install relevant libraries without affecting base environment.</p> <p>Run <code class="language-plaintext highlighter-rouge">conda create -n zoom python-3.9</code> where zoom is the environment name, you can name anything you want.</p> <p>Then activate the environment by running <code class="language-plaintext highlighter-rouge">conda activate </code></p> <p><strong><code class="language-plaintext highlighter-rouge">Step 2</code></strong></p> <p>Create requirements.txt file which contains all the relevant libraries which we will be using for this lesson.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pandas==1.5.2
prefect==2.7.7
prefect-sqlalchemy==0.2.2
prefect-gcp[cloud_storage]==0.2.4
protobuf==4.21.11
pyarrow==10.0.1
pandas-gbq==0.18.1
psycopg2-binary==2.9.5
sqlalchemy==1.4.46
</code></pre></div></div> <p>Run <code class="language-plaintext highlighter-rouge">pip install -r requirement.txt</code> which will load all the libraries in the zoom environment.</p> <p><strong><code class="language-plaintext highlighter-rouge">Step 3</code></strong></p> <p>Now, lets transform the ingest_data.py file from week 1 into flows and tasks. we can use this concept of task and flow to break the ingest_data python script into multiple tasks and flows which would help us visualize our whole workflow better.</p> <ol> <li>Load the necessary libraries:</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">argparse</span>
<span class="kn">from</span> <span class="n">time</span> <span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">sqlalchemy</span> <span class="kn">import</span> <span class="n">create_engine</span>
<span class="kn">from</span> <span class="n">prefect</span> <span class="kn">import</span> <span class="n">flow</span><span class="p">,</span> <span class="n">task</span>
<span class="kn">from</span> <span class="n">prefect.tasks</span> <span class="kn">import</span> <span class="n">task_input_hash</span>
<span class="kn">from</span> <span class="n">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span>
</code></pre></div></div> <ol> <li>Create an extract function which will help extract data from the given url:</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@task</span><span class="p">(</span><span class="n">log_prints</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">retries</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">cache_key_fn</span><span class="o">=</span><span class="n">task_input_hash</span><span class="p">,</span> <span class="n">cache_expiration</span><span class="o">=</span><span class="nf">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">extract_data</span><span class="p">(</span><span class="n">csv_url</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">csv_url</span><span class="p">.</span><span class="nf">endswith</span><span class="p">(</span><span class="sh">'</span><span class="s">.csv.gz</span><span class="sh">'</span><span class="p">):</span>
        <span class="n">csv_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">yellow_tripdata_2021-01.csv.gz</span><span class="sh">'</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">csv_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">output.csv</span><span class="sh">'</span>

    <span class="n">os</span><span class="p">.</span><span class="nf">system</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">wget </span><span class="si">{</span><span class="n">csv_url</span><span class="si">}</span><span class="s"> -O </span><span class="si">{</span><span class="n">csv_name</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">df_iter</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="n">csv_name</span><span class="p">,</span> <span class="n">iterator</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">chunksize</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>

    <span class="n">df</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="n">df_iter</span><span class="p">)</span>

    <span class="n">df</span><span class="p">.</span><span class="n">tpep_pickup_datetime</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">tpep_pickup_datetime</span><span class="p">)</span>
    <span class="n">df</span><span class="p">.</span><span class="n">tpep_dropoff_datetime</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">tpep_dropoff_datetime</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">df</span>
</code></pre></div></div> <ol> <li>Create a transform function to transform the data:</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@task</span><span class="p">(</span><span class="n">log_prints</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">transform_data</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">pre: missing passenger count: </span><span class="si">{</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">passenger_count</span><span class="sh">'</span><span class="p">].</span><span class="nf">isin</span><span class="p">([</span><span class="mi">0</span><span class="p">]).</span><span class="nf">sum</span><span class="p">()</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">passenger_count</span><span class="sh">'</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">post: missing passenger count: </span><span class="si">{</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">passenger_count</span><span class="sh">'</span><span class="p">].</span><span class="nf">isin</span><span class="p">([</span><span class="mi">0</span><span class="p">]).</span><span class="nf">sum</span><span class="p">()</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">df</span>
</code></pre></div></div> <ol> <li>Next we create load function(ingest_data) to load the data:</li> </ol> <p>Here we use a concepts of block to store configuration and provide an interface of interacting with external systems.</p> <p>In our ingest_data.py, instead of hard coding all the input credentials(url, user, passwords) as shown in below commented code we can create a block which can store the credentials and can be called directly.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/data//week2/block1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/data//week2/block1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/data//week2/block1-1400.webp"></source> <img src="/assets/img/blog/data//week2/block1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/data/week2/block2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/data/week2/block2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/data/week2/block2-1400.webp"></source> <img src="/assets/img/blog/data/week2/block2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/data/week2/block3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/data/week2/block3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/data/week2/block3-1400.webp"></source> <img src="/assets/img/blog/data/week2/block3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@task</span><span class="p">(</span><span class="n">log_prints</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">retries</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">ingest_data</span><span class="p">(</span><span class="n">table_name</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>

    <span class="n">connection_block</span> <span class="o">=</span> <span class="n">SqlAlchemyConnector</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">'</span><span class="s">postgres-connector</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">connection_block</span><span class="p">.</span><span class="nf">get_connection</span><span class="p">(</span><span class="n">begin</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">engine</span><span class="p">:</span>
        <span class="n">df</span><span class="p">.</span><span class="nf">head</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">to_sql</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">table_name</span><span class="p">,</span> <span class="n">con</span><span class="o">=</span><span class="n">engine</span><span class="p">,</span> <span class="n">if_exists</span><span class="o">=</span><span class="sh">'</span><span class="s">replace</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">df</span><span class="p">.</span><span class="nf">to_sql</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">table_name</span><span class="p">,</span> <span class="n">con</span><span class="o">=</span><span class="n">engine</span><span class="p">,</span> <span class="n">if_exists</span><span class="o">=</span><span class="sh">'</span><span class="s">replace</span><span class="sh">'</span><span class="p">)</span>

        <span class="c1"># postgres_url = f'postgresql://{user}:{password}@{host}:{port}/{db}'
</span>        <span class="c1"># engine = create_engine(postgres_url)
</span>
        <span class="c1"># df.head(n=0).to_sql(name=table_name, con=engine if_exists='replace')
</span>        <span class="c1"># df.to_sql(name=table_name, con=engine, if_exists='append')
</span></code></pre></div></div> <ol> <li>Finally, we create a main function which will help us run all of these functions</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@flow</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">Ingest Flow</span><span class="sh">"</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">table_name</span><span class="p">:</span><span class="nb">str</span><span class="p">):</span>
    <span class="c1"># user = "root"
</span>    <span class="c1"># password = "root"
</span>    <span class="c1"># host = "localhost"
</span>    <span class="c1"># port = "5432"
</span>    <span class="c1"># db = "ny_taxi"
</span>    <span class="n">csv_url</span> <span class="o">=</span> <span class="sh">"</span><span class="s">https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz</span><span class="sh">"</span>

    <span class="nf">log_subflow</span><span class="p">(</span><span class="n">table_name</span><span class="p">)</span>
    <span class="n">raw_data</span> <span class="o">=</span> <span class="nf">extract_data</span><span class="p">(</span><span class="n">csv_url</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="nf">transform_data</span><span class="p">(</span><span class="n">raw_data</span><span class="p">)</span>
    <span class="nf">ingest_data</span><span class="p">(</span><span class="n">table_name</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="nf">main</span><span class="p">(</span><span class="sh">"</span><span class="s">yellow_taxi_trips</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong><code class="language-plaintext highlighter-rouge">Step 4</code></strong></p> <p>Save this file as <code class="language-plaintext highlighter-rouge">ingest_data_flow.py</code> and run the file through GitBash by using the command <code class="language-plaintext highlighter-rouge">python ingest_data_flow.py</code></p> <p>This should successfully load the data into postgres. But since we have only given the first chunk, it wont load the full dataset. As you can see below, since we set log_prints as True, we can clearly see the logs for each function. Also as you can see, the number of records is ~100,000 as only the first chunk is loaded. [185 records with 0 passenger count was removed. Check the transform function in the code above]</p> <h3 id="etl-to-gcp">ETL to GCP</h3> <p><strong><code class="language-plaintext highlighter-rouge">Step 1</code></strong>: Register the block with command</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>prefect block register <span class="nt">-m</span> prefect_gcp

</code></pre></div></div> <p><strong><code class="language-plaintext highlighter-rouge">Step 2</code></strong> : Create a google cloud storage bucket block connector</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/data/week2/gcs1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/data/week2/gcs1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/data/week2/gcs1-1400.webp"></source> <img src="/assets/img/blog/data/week2/gcs1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong><code class="language-plaintext highlighter-rouge">Step 3</code></strong>: Add GCS credential</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/data/week2/gcs2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/data/week2/gcs2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/data/week2/gcs2-1400.webp"></source> <img src="/assets/img/blog/data/week2/gcs2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong><code class="language-plaintext highlighter-rouge">Step 4</code></strong> : ETL from gcs to bq</p> <p>To write to gsc</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@task</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">write_gcs</span><span class="p">(</span><span class="n">path</span><span class="p">:</span><span class="n">Path</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Upload the parquet file to gcs</span><span class="sh">"""</span>

    <span class="n">gcp_storage_block</span> <span class="o">=</span> <span class="n">GcsBucket</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">zoom-gcs</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">gcp_storage_block</span><span class="p">.</span><span class="nf">upload_from_path</span><span class="p">(</span><span class="n">from_path</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="n">to_path</span><span class="o">=</span><span class="n">path</span><span class="p">)</span>
    <span class="k">return</span>
</code></pre></div></div> <p>To load data to gcs we follow the same step as from loading to postgres and adjusted our code when necessary.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">from</span> <span class="n">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">prefect</span> <span class="kn">import</span> <span class="n">flow</span><span class="p">,</span> <span class="n">task</span>
<span class="kn">from</span> <span class="n">prefect_gcp.cloud_storage</span> <span class="kn">import</span> <span class="n">GcsBucket</span>
<span class="kn">from</span> <span class="n">random</span> <span class="kn">import</span> <span class="n">randint</span>
<span class="kn">from</span> <span class="n">prefect_gcp.cloud_storage</span> <span class="kn">import</span> <span class="n">GcsBucket</span>

<span class="nd">@task</span><span class="p">(</span><span class="n">retries</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">fetch</span><span class="p">(</span><span class="n">dataset_url</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Read taxi data from web into pandas DataFrame</span><span class="sh">"""</span>

    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="n">dataset_url</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span>

<span class="nd">@task</span><span class="p">(</span><span class="n">log_prints</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">clean</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Fix dtypes issues</span><span class="sh">"""</span>
    <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">tpep_pickup_datetime</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">tpep_pickup_datetime</span><span class="sh">'</span><span class="p">])</span>
    <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">tpep_dropoff_datetime</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">tpep_dropoff_datetime</span><span class="sh">'</span><span class="p">])</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="nf">head</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">columns: </span><span class="si">{</span><span class="n">df</span><span class="p">.</span><span class="n">dtypes</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">rows: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span>

<span class="nd">@task</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">write_local</span><span class="p">(</span><span class="n">df</span><span class="p">:</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">color</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> <span class="n">dataset_file</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Path</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Write Dataframe out as parquet file</span><span class="sh">"""</span>

    <span class="n">path</span> <span class="o">=</span> <span class="nc">Path</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">../data/</span><span class="si">{</span><span class="n">color</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">dataset_file</span><span class="si">}</span><span class="s">.parquet</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">df</span><span class="p">.</span><span class="nf">to_parquet</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="sh">"</span><span class="s">gzip</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">path</span>


<span class="nd">@task</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">write_gcs</span><span class="p">(</span><span class="n">path</span><span class="p">:</span><span class="n">Path</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Upload the parquet file to gcs</span><span class="sh">"""</span>

    <span class="n">gcp_storage_block</span> <span class="o">=</span> <span class="n">GcsBucket</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">zoom-gcs</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">gcp_storage_block</span><span class="p">.</span><span class="nf">upload_from_path</span><span class="p">(</span><span class="n">from_path</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="n">to_path</span><span class="o">=</span><span class="n">path</span><span class="p">)</span>
    <span class="k">return</span>

<span class="nd">@flow</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">etl_web_to_gcs</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">The Main ETL Function</span><span class="sh">"""</span>
    <span class="n">color</span> <span class="o">=</span> <span class="sh">"</span><span class="s">yellow</span><span class="sh">"</span>
    <span class="n">year</span> <span class="o">=</span> <span class="mi">2021</span>
    <span class="n">month</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">dataset_file</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">color</span><span class="si">}</span><span class="s">_tripdata_</span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="s">-</span><span class="si">{</span><span class="n">month</span><span class="si">:</span><span class="mi">02</span><span class="si">}</span><span class="sh">"</span>
    <span class="n">dataset_url</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">https://github.com/DataTalksClub/nyc-tlc-data/releases/download/</span><span class="si">{</span><span class="n">color</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">dataset_file</span><span class="si">}</span><span class="s">.csv.gz</span><span class="sh">"</span>

    <span class="n">df</span> <span class="o">=</span> <span class="nf">fetch</span><span class="p">(</span><span class="n">dataset_url</span><span class="p">)</span>
    <span class="n">df_clean</span> <span class="o">=</span> <span class="nf">clean</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="n">path</span> <span class="o">=</span> <span class="nf">write_local</span><span class="p">(</span><span class="n">df_clean</span><span class="p">,</span> <span class="n">color</span><span class="p">,</span> <span class="n">dataset_file</span><span class="p">)</span>
    <span class="nf">write_gcs</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="nf">etl_web_to_gcs</span><span class="p">()</span>

</code></pre></div></div> <h3 id="prefect-deployment">Prefect Deployment</h3> <p>Prefect is a workflow management system designed to help data engineers and data scientists automate, schedule, and monitor data workflows. Deploying a Prefect workflow involves setting it up to run in a production environment, allowing it to be executed on a schedule or triggered by external events.</p> <h4 id="using-prefect-cli">Using Prefect CLI</h4> <p>There are two methods to deploy the flow: CLI or python file.</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>prefect deployment build ./parameterized_flow.py:etl_parent_flow <span class="nt">-n</span> <span class="s2">"Parameterized ETL"</span>
</code></pre></div></div> <ul> <li> <p>Command: <code class="language-plaintext highlighter-rouge">prefect deployment build</code>: This Prefect CLI command is used to prepare the settings for a deployment. The build command likely indicates that it’s configuring and setting up the deployment environment.</p> </li> <li> <p>./parameterized_flow.py:etl_parent_flow: This part of the command specifies the location of the Prefect flow script file and the name of the entrypoint flow function. In this case, the script is parameterized_flow.py, and the entrypoint flow function is etl_parent_flow. The format is file_path:entrypoint_function.</p> </li> <li> <p>-n log-simple: This part of the command uses the -n flag to specify a name for the deployment. In this case, the name provided is log-simple. Naming deployments can be helpful for identification and management.</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/data/week2/deploy3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/data/week2/deploy3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/data/week2/deploy3-1400.webp"></source> <img src="/assets/img/blog/data/week2/deploy3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>As you can see from the output that prefect deployment build command generates a YAML file defining a Prefect deployment. It includes workflow details (file, object name), deployment info (name, version, environment), configuration values, and metadata (creation timestamp, Prefect version). This YAML file is input for deploying or updating workflows via Prefect CLI or API, allowing easy management and modification of deployments.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/data/week2/deploy1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/data/week2/deploy1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/data/week2/deploy1-1400.webp"></source> <img src="/assets/img/blog/data/week2/deploy1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/data/week2/deploy2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/data/week2/deploy2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/data/week2/deploy2-1400.webp"></source> <img src="/assets/img/blog/data/week2/deploy2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Before deploying, we can edit some of the work queue in the YAML file to add more details like Parameters, schedules, etc.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/data/week2/deploy4-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/data/week2/deploy4-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/data/week2/deploy4-1400.webp"></source> <img src="/assets/img/blog/data/week2/deploy4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Then we can use the following command to apply the deployment</p> <p><code class="language-plaintext highlighter-rouge">prefect deployment apply etl_parent_flow-deployment.yaml</code></p> <p>This action involves deploying the workflow described in the YAML file to the target environment. The specific target environment could be either a Prefect Cloud instance or a Prefect Server, depending on the configuration details specified within the YAML file.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/data/week2/deploy5-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/data/week2/deploy5-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/data/week2/deploy5-1400.webp"></source> <img src="/assets/img/blog/data/week2/deploy5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>If you navigate to the prefect UI, you can see that the deployment has been created:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/data/week2/deploy6-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/data/week2/deploy6-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/data/week2/deploy6-1400.webp"></source> <img src="/assets/img/blog/data/week2/deploy6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>There are 2 types of run -</p> <p><code class="language-plaintext highlighter-rouge">Quick Run</code> - Simpler way to run a workflow, providing only a limited set of options. The inputs and environment variables are pre-configured based on the default values specified in the workflow, and you cannot specify additional options or modify the inputs and environment variables.</p> <p><code class="language-plaintext highlighter-rouge">Custom Run</code> - Allows you to manually specify the input parameters and environment variables for a run, and provides greater control over the execution of the workflow. You can also specify advanced run options, such as the run schedule and how long to retain logs and artifacts.</p> <p><strong>Agents and Work Queues</strong></p> <p>In Prefect, two key concepts for executing workflows in a distributed manner are “Work Queues” and “Agents.”</p> <p><code class="language-plaintext highlighter-rouge">Work Queues</code>:</p> <ul> <li>A Work Queue is a data structure that serves as a buffer between Prefect Core and the agents.</li> <li>It holds tasks that are ready to be executed, allowing Prefect Core to manage the tasks while agents pull and execute them.</li> <li>Work Queues facilitate a smooth flow of tasks between the workflow manager and the agents.</li> </ul> <p><code class="language-plaintext highlighter-rouge">Agents</code>:</p> <ul> <li>An Agent is a software component responsible for pulling tasks from the Work Queue and executing them.</li> <li>Agents can operate on various platforms, such as local machines, cloud infrastructure, or containers.</li> <li>They play a crucial role in the distributed execution of workflows.</li> <li>Agents can be deployed on a single node or in a cluster, working collaboratively to process tasks from the Work Queue.</li> <li>They contribute to the parallel and scalable execution of tasks by efficiently distributing the workload.</li> </ul> <p>Our Example</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/data/week2/deploy7-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/data/week2/deploy7-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/data/week2/deploy7-1400.webp"></source> <img src="/assets/img/blog/data/week2/deploy7.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We can then call the agent to pick up the workflow and execute it by using the following command -</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>prefect agent start <span class="nt">--work-queue</span> <span class="s2">"default"</span>
</code></pre></div></div> <h3 id="docker-infrastructure">Docker Infrastructure</h3> <ol> <li>Create docker image</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/data/week2/docker1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/data/week2/docker1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/data/week2/docker1-1400.webp"></source> <img src="/assets/img/blog/data/week2/docker1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ol> <li>Run the command <code class="language-plaintext highlighter-rouge">docker build -t &lt;hub-user&gt;/&lt;repo-name&gt;[:&lt;tag&gt;] .</code> -&gt; <code class="language-plaintext highlighter-rouge">docker image build -t k2ki/prefect:zoom .</code> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/data/week2/docker2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/data/week2/docker2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/data/week2/docker2-1400.webp"></source> <img src="/assets/img/blog/data/week2/docker2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ol> <li> <p>push it into the hub so that it can be used <code class="language-plaintext highlighter-rouge">docker image push k2ki/prefect:zoom</code></p> </li> <li> <p>Create docker block in Prefect UI. Edit the field accordingly.</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/data/week2/docker3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/data/week2/docker3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/data/week2/docker3-1400.webp"></source> <img src="/assets/img/blog/data/week2/docker3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="prefect-deployment-1">prefect deployment</h4> <ol> <li>write the python code</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">prefect.deployments</span> <span class="kn">import</span> <span class="n">Deployment</span>
<span class="kn">from</span> <span class="n">prefect.infrastructure.container</span> <span class="kn">import</span> <span class="n">DockerContainer</span>
<span class="kn">from</span> <span class="n">parameterized_flow1</span> <span class="kn">import</span> <span class="n">etl_parent_flow</span>

<span class="n">docker_block</span> <span class="o">=</span> <span class="n">DockerContainer</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">zoom</span><span class="sh">"</span><span class="p">)</span>

<span class="n">docker_dep</span> <span class="o">=</span> <span class="n">Deployment</span><span class="p">.</span><span class="nf">build_from_flow</span><span class="p">(</span><span class="n">flow</span><span class="o">=</span><span class="n">etl_parent_flow</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">docker-flow</span><span class="sh">"</span><span class="p">,</span> <span class="n">infrastructure</span><span class="o">=</span><span class="n">docker_block</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="n">docker_dep</span><span class="p">.</span><span class="nf">apply</span><span class="p">()</span>
</code></pre></div></div> <ol> <li>Execute the code</li> </ol> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python docker_deploy.py
</code></pre></div></div> <ol> <li>Verify in prefect UI</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/data/week2/docker4-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/data/week2/docker4-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/data/week2/docker4-1400.webp"></source> <img src="/assets/img/blog/data/week2/docker4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ol> <li>Start an agent using <code class="language-plaintext highlighter-rouge">prefect agent start -q default</code> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/data/week2/docker5-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/data/week2/docker5-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/data/week2/docker5-1400.webp"></source> <img src="/assets/img/blog/data/week2/docker5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ol> <li>Run the docker flow we created from CLI by running the command <code class="language-plaintext highlighter-rouge">prefect deployment run etl-parent-flow/docker-flow</code> </li> </ol> <p>This should complete the workflow process load the data into GCS.</p> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0" style="text-align: center;"> © Copyright 2025 Kamal Patel. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>